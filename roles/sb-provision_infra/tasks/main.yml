---
# The SG allows SSH from "cidr_ip" to your instances. Default is *any*
- name: Ensure that SG with SSH inbound exist (for VM)
  ec2_group:
    aws_access_key: "{{ aws_access_key }}"
    aws_secret_key: "{{ aws_secret_key }}"
    region: "{{ aws_region }}"
    name: "{{ security_group_VM }}"
    description: SG for VMs
    rules:
      - proto: tcp
        ports:
        - 22
        cidr_ip: 0.0.0.0/0
        rule_desc: pass from any on port 22

# The SG allows TCP on port 80 from "cidr_ip" to your ELB. Default is *any*
- name: Ensure that SG with 80 inbound exist (for ELB)
  ec2_group:
    aws_access_key: "{{ aws_access_key }}"
    aws_secret_key: "{{ aws_secret_key }}"
    region: "{{ aws_region }}"
    name: "{{ security_group_ELB }}"
    description: SG for ELB
    rules:
      - proto: tcp
        ports:
        - 80
        cidr_ip: 0.0.0.0/0
        rule_desc: pass from any on port 80

- name: Create a new key pair if it not exist
  ec2_key:
    aws_access_key: "{{ aws_access_key }}"
    aws_secret_key: "{{ aws_secret_key }}"
    region: "{{ aws_region }}"
    name: "{{ keypair_name }}"
    force: false
  register: aws_key

- name: Save generated key in ssh home directory, but never overwrite
  copy:
    content: "{{ aws_key.key.private_key }}"
    dest: "~/.ssh/id_rsa_aws.{{ keypair_name }}.pem"
    mode: 0600
    force: no
  when: aws_key.changed

# "exact_count" of instances matching "count_tag" to be present.
# For some reasone exact_count in incompatible with started so a stopped instance will not start
- name: Launch EC2 instances
  ec2:
    aws_access_key: "{{ aws_access_key }}"  
    aws_secret_key: "{{ aws_secret_key }}"
    keypair: "{{ keypair_name }}"
    group: "{{ security_group_VM }},{{ security_group_VPC}}"
    instance_type: "{{ instance_type }}"
    image: "{{ ami_image }}"
    region: "{{ aws_region }}"
    count_tag:
      group: sb
    exact_count: "{{ ec2_count }}"
    instance_tags:
      group: sb
    wait: true
  register: ec2

# Register for other roles
- name: Add new instances to in-memory inventory
  add_host:
    hostname: "{{ item.public_dns_name }}"
    groups: svm_ec2
    ansible_ssh_private_key_file:  "~/.ssh/id_rsa_aws.{{ keypair_name }}.pem"
  with_items: "{{ ec2.tagged_instances }}"
  when: ec2.changed

- name: Wait for instances to start
  wait_for:
    host: "{{ item.public_dns_name }}"
    port: 22
    delay: 30
    timeout: 300
    state: started
  with_items: "{{ ec2.tagged_instances }}"
  when: ec2.changed

# Create load balancer and attach it to its SG
- name: Create the ELB
  ec2_elb_lb:
    aws_access_key: "{{ aws_access_key }}"  
    aws_secret_key: "{{ aws_secret_key }}"
    name: "{{ elb_name }}"
    security_group_names: "{{ security_group_ELB }},{{ security_group_VPC }}"
    region: "{{ aws_region }}"
    zones:
      - "{{ aws_region }}a"
    listeners:
      - protocol: tcp
        load_balancer_port: 80
        instance_port: 80
    state: present

# FIXME
# This task will fail cause the web app is not yet started and there is no way to disable or reanable HC on the ELB
# As a future improvement the task should be runned after the deployment of the web app
- name: Add instances to ELB
  ignore_errors: yes
  ec2_elb:
    aws_access_key: "{{ aws_access_key }}"  
    aws_secret_key: "{{ aws_secret_key }}"
    region: "{{ aws_region }}"
    ec2_elbs: "{{ elb_name }}"
    instance_id: "{{ item.id  }}"
    state: present
  with_items: "{{ ec2.tagged_instances }}"
